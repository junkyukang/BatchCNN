{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm as tqdm_auto\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#무작위성 제한\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.read_pickle(\"train_data.pickle\")\n",
    "test_data= pd.read_pickle(\"test_data.pickle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Batch CNN 사용</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tuned 모델은 첨부되어 있음\n",
    "num_label = 7\n",
    "path=\"JKKANG/ALBERT-kor-emotion\"\n",
    "albert = AutoModel.from_pretrained(path, num_labels= num_label).to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch 생성을 위한 \n",
    "def dataset(data):\n",
    "    save=[]\n",
    "    k=data['Segment ID'][0][0:15]\n",
    "    for i,seg in enumerate(data['Segment ID']):\n",
    "\n",
    "        if seg[7:13]=='script':\n",
    "            seg=seg[0:15]\n",
    "            if  k==seg:\n",
    "                k=seg\n",
    "                continue\n",
    "            else: \n",
    "                k=seg\n",
    "                save.append(i)\n",
    "        else:\n",
    "            seg=seg[0:14]\n",
    "            if  k==seg: \n",
    "                k=seg\n",
    "                continue\n",
    "            else: \n",
    "                k=seg\n",
    "                save.append(i)\n",
    "            \n",
    "    return save\n",
    "\n",
    "\n",
    "#train data\n",
    "train_load=dataset(train_data)\n",
    "\n",
    "train_dataset=[]\n",
    "start=0\n",
    "for i in train_load:\n",
    "    \n",
    "    train_dataset.append(train_data.loc[start:i])\n",
    "    start=i\n",
    "train_dataset.append(train_data.loc[start:])\n",
    "\n",
    "\n",
    "#test data\n",
    "test_load=dataset(test_data)\n",
    "\n",
    "test_dataset=[]\n",
    "start=0\n",
    "for i in test_load:\n",
    "    \n",
    "    test_dataset.append(test_data.loc[start:i])\n",
    "    start=i\n",
    "test_dataset.append(test_data.loc[start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch CNN 사용\n",
    "\n",
    "\n",
    "#Model\n",
    "class MY_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MY_Model, self).__init__()\n",
    " \n",
    "        \n",
    "        self.feature= nn.Sequential(\n",
    "            nn.Conv1d(768, 768,kernel_size=3, padding=1))\n",
    "        \n",
    "        self.audio= nn.Sequential(\n",
    "            nn.Linear(88,44),\n",
    "            nn.Linear(44, 22),\n",
    "            nn.Linear(22,7))\n",
    "        \n",
    "        self.feature_final= nn.Sequential(\n",
    "            nn.Linear(775, 775),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(775, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 7))\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, audio_data, text_data, albert, tokenizer):\n",
    "        \n",
    "        text_data= tokenizer(text_data,truncation=True,padding=True,return_token_type_ids=False, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad(): #albert는 fine-tuning 한 모델을 가져다 씀 \n",
    "            text_data = {k:v.to(device) for k,v in text_data.items()}\n",
    "            out_text= albert(**text_data)    \n",
    "           \n",
    "        out_text=out_text.last_hidden_state\n",
    "\n",
    "            \n",
    "        out=out_text[:,0]     \n",
    "        out1=torch.transpose(out,0,1)\n",
    "        out1=self.feature(out1)\n",
    "        out1= torch.transpose(out1,0,1)\n",
    "        \n",
    "        audio= self.audio(audio_data)\n",
    "        out2= torch.concat([out1,audio],dim=1)\n",
    "        \n",
    "        final= self.feature_final(out2)   \n",
    "        \n",
    "            \n",
    "        return final\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MY_Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4af40ddae1945fe95dd476f13e8e830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------1---------\n",
      "loss : 0.47856827569705185\n",
      "val acc:0.7427341227125942\n",
      "---------2---------\n",
      "loss : 0.2476697007150116\n",
      "val acc:0.7481162540365985\n",
      "---------3---------\n",
      "loss : 0.1908617793326357\n",
      "val acc:0.7502691065662002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kang\\Desktop\\BatchCNN\\model.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kang/Desktop/BatchCNN/model.ipynb#X65sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     cost\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kang/Desktop/BatchCNN/model.ipynb#X65sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kang/Desktop/BatchCNN/model.ipynb#X65sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     epoch_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mcost\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kang/Desktop/BatchCNN/model.ipynb#X65sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m epoch_loss\u001b[39m=\u001b[39mepoch_loss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_load)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kang/Desktop/BatchCNN/model.ipynb#X65sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss_history\u001b[39m.\u001b[39mappend(epoch_loss)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "optimizer= torch.optim.AdamW(model.parameters(), lr=0.001, amsgrad=True )\n",
    "loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "epochs = 10\n",
    "loss_history = []\n",
    "accuracy_history=[]\n",
    "best = {\"acc\": 0}\n",
    "\n",
    "for epoch in tqdm_auto(range(epochs)):\n",
    "    epoch_loss=0\n",
    "    for i in train_dataset:\n",
    "        text_data=list(i['text_data'])\n",
    "        audio_data=torch.tensor(i['audio_data'].reset_index(drop=True),dtype=torch.float32).to(device)\n",
    "        label= torch.tensor(i['label'].reset_index(drop=True),dtype=torch.long).to(device)\n",
    "        \n",
    "        # model 계산\n",
    "        preds = model(audio_data, text_data,  albert, tokenizer)\n",
    "        \n",
    "        # cost 계산\n",
    "        \n",
    "        \n",
    "        cost= loss(preds, label)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss+=cost.item()\n",
    "    \n",
    "    \n",
    "    epoch_loss=epoch_loss/len(train_load)\n",
    "    loss_history.append(epoch_loss)\n",
    "    \n",
    "    \n",
    "    print(f'---------{epoch+1}---------')\n",
    "    print(f'loss : {epoch_loss}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corrects=torch.tensor([]).to(device)\n",
    "    preds=torch.tensor([]).to(device)\n",
    "    target=torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in test_dataset:\n",
    "            text_v=list(i['text_data'])\n",
    "            audio_v=torch.tensor(i['audio_data'].reset_index(drop=True),dtype=torch.float32).to(device)\n",
    "            label_v= torch.tensor(i['label'].reset_index(drop=True),dtype=torch.long).to(device)\n",
    "            \n",
    "            out_v= model(audio_v, text_v, albert, tokenizer)\n",
    "            result_v= torch.argmax(out_v, dim=1)\n",
    "            correct_v= label_v==result_v\n",
    "            \n",
    "            corrects= torch.cat([corrects,correct_v],dim=0)\n",
    "            preds=torch.cat([preds,result_v],dim=0)\n",
    "            target=torch.cat([target, label_v],dim=0)\n",
    "    accuracy = corrects.sum().item() / len(corrects)\n",
    "    accuracy_history.append(accuracy)\n",
    "    if accuracy > best[\"acc\"]:\n",
    "        \n",
    "        best[\"state\"] = copy.deepcopy(model)\n",
    "        best[\"acc\"] = accuracy\n",
    "        best[\"epoch\"] = epoch + 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"val acc:{accuracy}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "torch.save(best['state'],'model_BatchCNN.pt')\n",
    "model2= torch.load('model_BatchCNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##성능 평가\n",
    "preds=torch.tensor([]).to(device)\n",
    "target=torch.tensor([]).to(device)\n",
    "    \n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    for i in test_dataset:\n",
    "        text_t=list(i['text_data'])\n",
    "        audio_t=torch.tensor(i['audio_data'].reset_index(drop=True),dtype=torch.float32).to(device)\n",
    "        label_t= torch.tensor(i['label'].reset_index(drop=True),dtype=torch.long).to(device)\n",
    "            \n",
    "        out_t= model2(audio_t, text_t, albert, tokenizer)\n",
    "        result_t= torch.argmax(out_t, dim=1)\n",
    "        \n",
    "        preds=torch.cat([preds,result_t],dim=0)\n",
    "        target=torch.cat([target, label_t],dim=0)\n",
    "    \n",
    "preds.int()\n",
    "target.int()\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(preds, target):\n",
    "    labels= target.tolist()\n",
    "    preds = preds.tolist()\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc , \"weighted f1\": weighted_f1, \"macro f1\": macro_f1}\n",
    "\n",
    "compute_metrics(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Batch CNN 사용 안함</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#batch CNN 사용\n",
    "\n",
    "\n",
    "#Model\n",
    "class MY_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MY_Model, self).__init__()\n",
    "\n",
    "        \n",
    "        self.audio= nn.Sequential(\n",
    "            nn.Linear(88,44),\n",
    "            nn.Linear(44, 22),\n",
    "            nn.Linear(22,7))\n",
    "        \n",
    "        self.feature_final= nn.Sequential(\n",
    "            nn.Linear(775, 775),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(775, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 7))\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, audio_data, text_data, albert, tokenizer):\n",
    "        \n",
    "        text_data= tokenizer(text_data,truncation=True,padding=True,return_token_type_ids=False, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad(): #albert는 fine-tuning 한 모델을 가져다 씀 \n",
    "            text_data = {k:v.to(device) for k,v in text_data.items()}\n",
    "            out_text= albert(**text_data)    \n",
    "           \n",
    "        out_text=out_text.last_hidden_state\n",
    "\n",
    "            \n",
    "        out=out_text[:,0]     \n",
    "        \n",
    "        \n",
    "        audio= self.audio(audio_data)\n",
    "        out2= torch.concat([out,audio],dim=1)\n",
    "        \n",
    "        final= self.feature_final(out2)   \n",
    "        \n",
    "            \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MY_Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71cef8dd619441fbab9bc2df274176c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------1---------\n",
      "loss : 0.38870653388322896\n",
      "val acc:0.7376715810879512\n",
      "---------2---------\n",
      "loss : 0.18953997458803518\n",
      "val acc:0.733604473817997\n",
      "---------3---------\n",
      "loss : 0.17370416933753116\n",
      "val acc:0.735638027452974\n",
      "---------4---------\n",
      "loss : 0.1693848330544491\n",
      "val acc:0.7397051347229283\n",
      "---------5---------\n",
      "loss : 0.15834349870060593\n",
      "val acc:0.7397051347229283\n",
      "---------6---------\n",
      "loss : 0.15436347905384668\n",
      "val acc:0.7391967463141841\n",
      "---------7---------\n",
      "loss : 0.14526111164006442\n",
      "val acc:0.7361464158617184\n",
      "---------8---------\n",
      "loss : 0.1451472968286206\n",
      "val acc:0.730045754956787\n",
      "---------9---------\n",
      "loss : 0.14162354969832255\n",
      "val acc:0.7397051347229283\n",
      "---------10---------\n",
      "loss : 0.1328808819827948\n",
      "val acc:0.7366548042704626\n",
      "---------11---------\n",
      "loss : 0.1317584620351076\n",
      "val acc:0.7346212506354856\n",
      "---------12---------\n",
      "loss : 0.1248557152680675\n",
      "val acc:0.7325876970005084\n",
      "---------13---------\n",
      "loss : 0.12648422254003816\n",
      "val acc:0.7305541433655313\n",
      "---------14---------\n",
      "loss : 0.12605111427558446\n",
      "val acc:0.7305541433655313\n",
      "---------15---------\n",
      "loss : 0.12232927752540297\n",
      "val acc:0.7285205897305541\n",
      "---------16---------\n",
      "loss : 0.13731132036430244\n",
      "val acc:0.7315709201830198\n",
      "---------17---------\n",
      "loss : 0.12237252722620344\n",
      "val acc:0.7295373665480427\n",
      "---------18---------\n",
      "loss : 0.11355102002536493\n",
      "val acc:0.7290289781392985\n",
      "---------19---------\n",
      "loss : 0.11841223098022848\n",
      "val acc:0.7341128622267412\n",
      "---------20---------\n",
      "loss : 0.11598482914367558\n",
      "val acc:0.7346212506354856\n",
      "---------21---------\n",
      "loss : 0.11047168340067906\n",
      "val acc:0.7285205897305541\n",
      "---------22---------\n",
      "loss : 0.11128981067002136\n",
      "val acc:0.7280122013218099\n",
      "---------23---------\n",
      "loss : 0.09972032773348317\n",
      "val acc:0.7366548042704626\n",
      "---------24---------\n",
      "loss : 0.108603574204218\n",
      "val acc:0.72089476359939\n",
      "---------25---------\n",
      "loss : 0.11622260201025017\n",
      "val acc:0.7341128622267412\n",
      "---------26---------\n",
      "loss : 0.10670526857694335\n",
      "val acc:0.7320793085917641\n",
      "---------27---------\n",
      "loss : 0.09815111312008865\n",
      "val acc:0.7325876970005084\n",
      "---------28---------\n",
      "loss : 0.09937455977496466\n",
      "val acc:0.730045754956787\n",
      "---------29---------\n",
      "loss : 0.1101107339101828\n",
      "val acc:0.7295373665480427\n",
      "---------30---------\n",
      "loss : 0.1067440823849817\n",
      "val acc:0.7280122013218099\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "optimizer= torch.optim.AdamW(model.parameters(), lr=0.001, amsgrad=True )\n",
    "loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "epochs = 10\n",
    "loss_history = []\n",
    "accuracy_history=[]\n",
    "best = {\"acc\": 0}\n",
    "\n",
    "for epoch in tqdm_auto(range(epochs)):\n",
    "    epoch_loss=0\n",
    "    for i in train_dataset:\n",
    "        text_data=list(i['text_data'])\n",
    "        audio_data=torch.tensor(i['audio_data'].reset_index(drop=True),dtype=torch.float32).to(device)\n",
    "        label= torch.tensor(i['label'].reset_index(drop=True),dtype=torch.long).to(device)\n",
    "        \n",
    "        # model 계산\n",
    "        preds = model(audio_data, text_data,  albert, tokenizer)\n",
    "        \n",
    "        # cost 계산\n",
    "        \n",
    "        \n",
    "        cost= loss(preds, label)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss+=cost.item()\n",
    "    \n",
    "    \n",
    "    epoch_loss=epoch_loss/len(train_load)\n",
    "    loss_history.append(epoch_loss)\n",
    "    \n",
    "    \n",
    "    print(f'---------{epoch+1}---------')\n",
    "    print(f'loss : {epoch_loss}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    corrects=torch.tensor([]).to(device)\n",
    "    preds=torch.tensor([]).to(device)\n",
    "    target=torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in test_dataset:\n",
    "            text_v=list(i['text_data'])\n",
    "            audio_v=torch.tensor(i['audio_data'].reset_index(drop=True),dtype=torch.float32).to(device)\n",
    "            label_v= torch.tensor(i['label'].reset_index(drop=True),dtype=torch.long).to(device)\n",
    "            \n",
    "            out_v= model(audio_v, text_v, albert, tokenizer)\n",
    "            result_v= torch.argmax(out_v, dim=1)\n",
    "            correct_v= label_v==result_v\n",
    "            \n",
    "            corrects= torch.cat([corrects,correct_v],dim=0)\n",
    "            preds=torch.cat([preds,result_v],dim=0)\n",
    "            target=torch.cat([target, label_v],dim=0)\n",
    "    accuracy = corrects.sum().item() / len(corrects)\n",
    "    accuracy_history.append(accuracy)\n",
    "    if accuracy > best[\"acc\"]:\n",
    "        \n",
    "        best[\"state\"] = copy.deepcopy(model)\n",
    "        best[\"acc\"] = accuracy\n",
    "        best[\"epoch\"] = epoch + 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"val acc:{accuracy}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
